



## CREATING THE DATABASE WITH create_db.py WORKS FINE.

## CREATING THE TABLES WITH create_tabels.py WORKES FINE.

## ITS THE Populate.py SCRIPT THAT WORKS NOW, FOR POPULATING THE DATABASE.



## **CHECKPOINT**


The initial connection and creation of the database works fine.
Even the population of the database.


the files that are relevant as of today: **08/03/24** are:
create_db.py
create_table.py
populate.py
database config.ini


these all talk very well with postgres and each other. 
    basic loggin functions for the reequest hase been in+mplemented and works fine. 

**THE FRAMEWORK**
Is a straight copy of the GP file with some key constraints.
This is because I want the inital functions to work before the schema becomes more complex.


**NEXT STEP:**
[X] feed all of GP data to the database. 
    [ ] this needs to be verified somehow.

[ ] establish update script. 
    The script that is most relevant right now is the **automatic_update_testnew.** 
    the script that works: 
    create_db
    create_table 
    populate 
    These tre talk and work with eachoter... Well the dont talk to eachoter yet but they all build on eachotehr.. 

    The problem is to get the update to work, and how can i see that it is updating, THEN we can focus on storing the old data. 
    
**Checkpoint** 
Roll back. 
Start over with new script. 
the scripts in **automatic_update_testnew.py** runs but throws a error related to a NORAD_CAT_ID and a TLE. I do not fully understand what the script does
    So I think I have to take a step back from that... 

The script **automatic_update.py** works BUT it says: // New data inserted into gp_file
Error inserting data into gp_file: insert or update on table "gp_file_historical" violates foreign key constraint "gp_file_historical_gp_file_id_fkey"
DETAIL:  Key (gp_file_id)=(20001) is not present in table "gp_file". //
i also noticed that it acutally imported on input... So it kinda works but hte violations is weird... I need it to update and not insert.. 
NExt step would be to first make sur the data from gp_file ends up in history and then the data ini gp_file can be updated.... 

## I AM MOVING FORWARD AND ROLLING BACK TO ONLY TESTING THE UPDATE AND NOT THE HISTORICAL STORING. 
I was worried that, how would I know what data and when it got updated, BUT the timestamp will tell me!! 
Because it only adds when it arrives in the database!! PERFEKT. 
Still rolling back to only test a update of data. 


**START 15/03/2024** 
Jobbar med att försöka få updateringen att fungera som den ska. 
i **update.py** det fungerar att hämta och printa ut det som det ska i terminalen. 

    [x] Jag får inte loopen att köra för det finns för många close.conn() i de andra functionerna. felsök och ta bort för att öppna ! 
        - Tog bort conn.close i update_postgres_data()

        [] jag tror queryn är fel, den hämtar all data från NORAD_CAT_ID 20000 jsut nu så då börajr iterationen om från den och avbryts kanske när den hittar uppdatering.? 
        # /class/gp/decay_date/null-val/epoch/%3Enow-30/orderby/norad_cat_id/format/json


    [] Ett problem är att den inte tycks updatera i databasen. Hur kan jag ta reda på det? 






**checkpoint**

Vad vi vet: 
    update_new.py updaterar data. 
        Vilken data vet vi inte... Det skulle lika gäran kunna vara bara timestamp
            MEN scriptet säger att den hämtar olika mängder för varje körning av skriptet. 
                NÄSTAN. Om skriptet körs samma dag med någon timmes intervall hämtas samma mängd och updateras.. alltså samma värde för samma objekt 
                hämtas och skrivs över i databasen eftersom inget skript finns som chekar om datan som hämtas redan finns i databasen. Den skriver bara över samma data helt enkelt. 

                MEN varför hämtar den olika mängder data? (kolla loggen)
                
                    2024-03-21 17:24:48,387 - INFO - Data insertion into PostgreSQL successful.
                    2024-03-21 17:24:48,387 - INFO - Database connection closed.
                    2024-03-21 17:24:48,387 - INFO - Number of objects updated in database: 25985
                    2024-03-21 17:27:11,348 - INFO - Login successful
                    2024-03-21 17:27:14,432 - INFO - Data fetched successfully
                    2024-03-21 17:27:23,902 - INFO - Data insertion into PostgreSQL successful.
                    2024-03-21 17:27:23,903 - INFO - Database connection closed.
                    2024-03-21 17:27:23,903 - INFO - Number of objects updated in database: 25985
                    2024-03-22 10:00:54,429 - INFO - Login successful
                    2024-03-22 10:00:57,405 - INFO - Data fetched successfully
                    2024-03-22 10:01:06,653 - INFO - Data insertion into PostgreSQL successful.
                    2024-03-22 10:01:06,653 - INFO - Database connection closed.
                    2024-03-22 10:01:06,653 - INFO - Number of objects updated in database: 26001
                    2024-03-22 14:16:22,696 - INFO - Login successful
                    2024-03-22 14:16:26,255 - INFO - Data fetched successfully
                    2024-03-22 14:16:35,318 - INFO - Data insertion into PostgreSQL successful.
                    2024-03-22 14:16:35,318 - INFO - Database connection closed.
                    2024-03-22 14:16:35,318 - INFO - Number of objects updated in database: 26000
                    2024-03-22 15:18:46,886 - INFO - Login successful
                    2024-03-22 15:18:50,302 - INFO - Data fetched successfully
                    2024-03-22 15:19:00,700 - INFO - Data insertion into PostgreSQL successful.
                    2024-03-22 15:19:00,700 - INFO - Database connection closed.
                    2024-03-22 15:19:00,700 - INFO - Number of objects updated in database: 26000
                LOGGEN ÄR ÅTERSTÄLLD OCH DETTA ÄR HISTORIA. 


                loggen säger att antal data har hämtast och updaterats i databasen 
                    MEN om vi kollar i databsen via DBever så finns bara fåtalet TIMESTAMP registerade med olika antal coresponderadne data. INTERSSANT! 
                    STORLEKEN i MB har inte heller ökat efter de senaste updateringarna idag 22/03/24



**DATUM: 25/02/24**
**uppdrag**
Skapa en ny datbas
[X]  Ny databas gpdata2 
[ ] Query gp_history på spacetrack query builder. 
    [X] Query the gp_history. 
    Just make a query work with data from one object 2 days back in time. 
    [X] 10 days back in time. 

[ ] Mata gp_history med data för alla NORAD_CAT_ID 2 dagar bak i tiden. 




**uppdrag**
[ ] Logik för sparining av historisk data till gp_history innan update. 
    [ ] transfer data from gp  >  gp_history. 
        Potential logic: 
            data fron gp  > gp_history 
            ny data  >  gp 
            REPEAT.


**queryn som update_new.py använder.** 
/class/gp/decay_date/null-val/epoch/%3Enow-30/orderby/norad_cat_id/format/json
 this query fetches information about satellites' orbital parameters from the Space-Track API. 
 It specifically targets active satellites (those that have not yet decayed from orbit) 
 with epochs ***within*** the last 30 days and orders the retrieved data by their NORAD Catalog ID, returning the results in JSON format.
https://chat.openai.com/share/d80728d1-f30d-4ad4-9b6c-fe2d1d55ea54

**checkpoint**
Denna query: https://www.space-track.org/basicspacedata/query/class/gp_history/EPOCH/%3Enow-2/orderby/NORAD_CAT_ID%20asc/limit/10/emptyresult/show
Hämtar data från gp_history
med Epoch från idag och 2 dagar bakåt. sorterat efter NORAD_CAT_ID och begräsnat till 10 stycken NORAD. 


This query: https://www.space-track.org/basicspacedata/query/class/gp_history/EPOCH/%3Enow-2/orderby/NORAD_CAT_ID%20asc/emptyresult/show
Hämtar data från gp_history 
med epoch från idag och 2 dagar bakåt. Sorterar efter NORAD_CAT_ID utan begräsning. ( alltså borde den hämta 58,000 obejkt x2 )


**RENAMED update_new.py to update_gpdata.py**

Mystery: 
historiskt data måste vid den intiala bfolkningen av databasen vara identisk med det aktuella databas tabellen. 

När den aktuella databas tabellen får en updatering görs först ingeting eftersom den då blir aktuell och en updatering före historiska data tabellen. 

Precis innan en updatering görs, hämtar historiska data tabellen data från den aktuella data tabellen och...
Den aktuella databasen gör en updatering via API:et. 


# lÖST 27/03/24 
### datan måste kopieras från gp_file och matas in i gp_file_historical och id måste specificerat. 
Båda tables i gpdata2 är populerade. 
    Förstår inte NULL values i gp_file_id columnen. Kan ha och göra med hur datan lades in, i vilken ordning osv. 
Nästa steg är att updatera gp_file och sedan migrera uppdaterad datan till gp_file_history



**27/03/24** 
    Okej så att först insert i gp_file 
    och sedna insert gp_file_history utelämnar forigen key constraints.

        LOGIKEN VID SKAPANDET AV DB 
        Först skapas databsen. create_db.py 
        skapa tabels. create_table.py
        Mata gp_file(parent table) populate_gp_file.py
        Kopiera data från gp_file till gp_file_historical 
        Updatera gp_file 
        Kopiera data från gp_file med condition datum till gp_file_historical 
        updatera gp_file 
        loop

## STEGEN I SKAPELSEFASEN. 
1. create_db.py
2. create_table.py
3. populate_gp_file.py
4. populate_gp_file_historical.py
5. update_gpfile.py
6  migration.py
CHECKPOINT: såhär långt funkar allt.

**Checkpoint**
28/03/28
Nu funkar lagringen av data i gp_file_history. 
Logiken i den försat fasen(databas skapelse) är som följer. 


## **CREATE DB PHASE**
steg 1. create_db.py FUNKAR manuellt. 
steg 2. create_table.py FUNKAR manuellt. 
steg 3. populate_gp_file.py FUNKAR manuellt. 
steg 4. populate_gp_file_historical.py FUNKAR manuellt. 


## **MANUALLY MAINTAIN DB**
steg 1. update_gpfile.py
steg 2. migration.py 
    - tar datan från gp_file.table >>> all data med dagens datum(modification_timstamp) >>> gp_file_history.table


    