# requriments


**Checkpoint**

The hasing is working with the insertion of the api data.. 
I have now populated the databse with all NORAD without a decay-date. 

I will now try to either make a new file that runs once every minute to update the database on hte contiditon timestamp. 
    PROBLEMS? 
        Is using a new hasfunction in another file, adding to the risk of duplicates? 


## ABOUT HASLIB AND HASING 

The likelihood of generating duplicate hashes using the hashlib library in Python or any other hashing algorithm primarily depends on the quality and characteristics of the hash function itself, as well as the nature of the input data.

The hashlib library in Python provides interfaces to various secure hash and message digest algorithms. These algorithms, such as MD5, SHA-1, SHA-256, etc., are designed to produce unique hash values for different inputs. However, due to the finite range of hash values and the potential for collisions (different inputs producing the same hash value), there is always a theoretical possibility of generating duplicate hashes.

For cryptographic hash functions like SHA-256, the probability of accidental collisions is exceedingly low, especially when the input data is sufficiently varied and randomly distributed.

However, if you are concerned about the possibility of collisions, especially in scenarios where a large number of hashes are being generated, you can employ techniques like salting (adding unique random data to each input before hashing) or using longer hash functions to reduce the likelihood of collisions.

In summary, while it's theoretically possible to generate duplicate hashes, the likelihood is extremely low with secure hash functions like those provided by hashlib.




**checkpoint**
The update.py script is the go to script right now. 
    I have a consern that the script wont insert new data because of the hash... 
        I am not sure the logic works, that beeing, as soon as the fetch function get new data and asignes a hash that hash is uniq.. ? 
        Is this waterprofe and am I just overthinking it...  


## ANYWAY 



### FOR TOMORROW: 
**Check this link to start the day:** https://chat.openai.com/share/fa32fb27-47c5-4e17-8d9f-9b257dade4cd
**SCRIPTS THAT ARE RELEVANT**
fetch_insert_auto.py 
fetch_insert.py - works and can be used to try things or to populate without beeing stuck in a loop. 

At the bottom of the chat, thats where we at. 

We hade aproblem with the script running without a stop so I asked for clean up of hte script and improvement. 
**I think** start over fresh tomorow, sahve the script that works indefinitly but fix so you CAN terminte it befor moving on to testing the nimproved script.. 
[ ] test teh improved scritp. 


#### **also**
Check if the update works. Does it add in new data? Duplciates? logging is working?

## SOO..
**what we have** 
A database.
    one table with hashes to define changes.. 

A script that consatnly runs and inserts upddates if there is some.. 
    Kind of.. I am not sure how it works yet... Time will tell. 

**NOW**
Create a UI. Probably with flask...
https://chat.openai.com/share/ca5f327c-1004-4178-93f5-04894646553c
At the end of this chat we talk about flask..  




##################################################################################################################################################
##################################################################################################################################################
##################################################################################################################################################




TODAYS TOMMOROW TASKTS 

[ ] Make a sercheble funtion, UI probably.. or in consol... 
[ ] Flagging function, to get notification on when a object has been updated with new data.. 
[ ] Decay alert( kind of ) when data is more than 5 day old.. print statement... ? 
[ ] Monitor update script to catch exat time of creation_date form Space.track. 

## **TODAY 04/04/24**

The fetch_insert_auto_new.py is running in a loop now to fetch data RINGT NOW on the epoch constraint... 
    This might not be the best since it is the creation data of the gp file that is the main one they go by ...
    The query given by space track to get the lates data acording to peoch ois becasue the epoch can be planed in the future!" 

    But the creationg date is more general and takes all data that they uploded this day. 
    SO I am going to run the script with the **creationg date = now** on another database. To se witch one does the best thing. 
**ALSO**
I must see if new data is beeing inserten and just not overrun in with the epoch query... 

**ALSO** 
I need to make the scritp run without it beeing terminated by exiting the VCS.


**Mission** 
Create functions for the UI 
    Flagging.
    decay data. 
    Search. 





###  Möte med Anton! 

fast API skapa den! 

egen kontainert eller hle intergread i schedulern? 

Fristående känns rimlig. 

Skissa upp endpoints skapa ett restfull api. 

Länken som anton skickade i teams. 
filerna som är interssant: backend server och database client.
rad 104 i backendserver. 

#### **Konklusion av samtalet.** 
börja skissa på endpoints till din databas. 
använd fast API 
skapa functionerna. 


**checkpoint**
Gällande BACKEND. 
två scripten: 
fetch_insert_auto_db2.py
    Denna ska köras varje minut. Queryn måste annpassa så den letar data som skapats senaste minuten. 

fetch_insert_auto_db3.py 
    Denna ska köras 3 gågner om dagen. och hämta data. 

Jag måste ta reda på om datan lagras med samma NORAD osv.. så som den gjort i gamla skript utan problem.... 


En paniklösning är att hämta data från gp_history från 5 dagar bakot för att ha lite att jobba med när vi skriver endpoints och functioner. 

**problem** 
Med gpdata2 och query i fetch_insert_auto_db2.py 
    Den skriv bara över varje norad med samma data när den hämtade det... 
I loggen insertion2.log ser vi hur många gånger samma object hämtats och skrivits över vilket ledde till att dens ID blev plussat i databasen. 


## **STATUS 04/04/24**
Har ändrat i GP_DATABASE_AUTO_DEV 
    Filerna I api/ är det som gäller.  
**populate_gpdata2.py** är den som kördes för att populera gpdata2 databasen. 

**insert_gpdata2.py** är den som ska bli automatisk och köras i oändligheten. 
    Den har nu en query som hämtar data baserat på creation_date som skapat de senaste 12 h. (0.5) 
    I morgon kommer jag köra den igen och se om den skriver över eller lägger in den nya datan BASERAT PÅ om HASHEN har ändrats. 
        Scriptet ska funka så att den hämtar all hash och om datan från requesten som kommer in har samma hash uppdateras den inte. 


**insert_gpdata2_auto.py**
Denna ska köras varje minut. med en query som tittar 5 minuter bakåt. 


###################################################################################################
**checkpoint time: 12:48 05/05/24**
Bytt namn på filer. 
Databsen heter orbital_info_bd

Databasen har befolkats av populate_orbital_info.py som änvander en query som hämtar all data som inte har decay value. (alltså decay_value = null )
    Därav de sjukt gamla creation date. 

Efter det körs insert_orbital_info_auto.py som har en query som hämtar data med creation date från de senaste 5 minuterna. 
    Där av är loggen jsut nu tom eftersom inga obejct skapat de senaste 5 minuterna. 

**MÅL**

Se till att insert_orbital_info_auto.py körs automatiskt i oändligheten. 
    Vad händer omden bryts? logg för när den bröts?? 
    
Se till att ett skript hämtar data de senaste 12h timmarna om loopen bryts. 

Skapa restAPI 
och basic funtioner 
endpoints. 



2024-04-06 11:28:13,668 - ERROR - Failed to login to space-track.org: HTTPSConnectionPool(host='www.space-track.org', port=443): Max retries exceeded with url: /ajaxauth/login (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000026930E5E8E0>: Failed to resolve 'www.space-track.org' ([Errno 11001] getaddrinfo failed)"))
2024-04-06 11:28:13,668 - ERROR - An error occurred: HTTPSConnectionPool(host='www.space-track.org', port=443): Max retries exceeded with url: /ajaxauth/login (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000026930E5E8E0>: Failed to resolve 'www.space-track.org' ([Errno 11001] getaddrinfo failed)"))

This happend in the code whne i ran it for many houures. GOOGLE IT! Culd it be becaose of limit requests ? 

2024-04-06 13:46:25,764 - INFO - Satellite data fetched successfully.
2024-04-06 13:46:25,810 - INFO - Inserted 0 data points successfully.
2024-04-06 13:46:25,811 - INFO - Skipped 0 duplicate data points.
2024-04-06 13:47:25,833 - ERROR - Failed to login to space-track.org: HTTPSConnectionPool(host='www.space-track.org', port=443): Max retries exceeded with url: /ajaxauth/login (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000026930D94EE0>: Failed to resolve 'www.space-track.org' ([Errno 11001] getaddrinfo failed)"))
2024-04-06 13:47:25,834 - ERROR - An error occurred: HTTPSConnectionPool(host='www.space-track.org', port=443): Max retries exceeded with url: /ajaxauth/login (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000026930D94EE0>: Failed to resolve 'www.space-track.org' ([Errno 11001] getaddrinfo failed)"))
2024-04-06 13:48:36,934 - ERROR - Failed to login to space-track.org: HTTPSConnectionPool(host='www.space-track.org', port=443): Max retries exceeded with url: /ajaxauth/login (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000026930E5E370>: Failed to resolve 'www.space-track.org' ([Errno 11001] getaddrinfo failed)"))
2024-04-06 13:48:36,935 - ERROR - An error occurred: HTTPSConnectionPool(host='www.space-track.org', port=443): Max retries exceeded with url: /ajaxauth/login (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000026930E5E370>: Failed to resolve 'www.space-track.org' ([Errno 11001] getaddrinfo failed)"))
2024-04-06 13:49:38,099 - INFO - Logged in successfully to space-track.org.
2024-04-06 13:49:38,886 - INFO - Satellite data fetched successfully.
2024-04-06 13:49:38,932 - INFO - Inserted 0 data points successfully.
2024-04-06 13:49:38,932 - INFO - Skipped 0 duplicate data points.
2024-04-06 13:50:41,041 - INFO - Logged in successfully to space-track.org.

then the same some 100 requests later maybe ? ....


## **IDAG 08/04/24**

Hur får man ett skript att köra även fast datorn stängs av? 

    Titta på erroren från loggen, vad betyder den? Går det att lösa? 


Visualisera datan, vilken tid på dygnet brukar datan skapas på spacetrack? (använd creation_date)


RENSA REQUIRMENTS LIST OCH SE ATT DEN KAN KÖRAS PÅ TEX, min stationära dataor... 

REST API! 



### **IDAG 09/04/24** 

Skriptet som gäller nu för det evigt rullande requestes and inserts är fetch_insert.py I api_requests. 
Denna mapp ska vi lägga i BACKEND tänker jag tillsammans med databas mappen och config. 


**Forstätt skapa endpoints**
Du håller på med en user. SKAPA users. https://chat.openai.com/share/8e4d55b6-0b86-45db-a3ad-0c23187596ca LÄNGST NER PÅ DENNA LÄNK ÄR DU! 
    Men hur funkar det? **-Jag vill att man ska behöva logga in för att få tillgång till datan först**

**Sökfunktion.** 
skapa det.

**alerts functions**

**flagging** giving alerts on specific conditions. 

**vart lever loggen?** 

**Anpassa projektmappen och gör den redo på deployment.** 
    Testa att deploya den på din stationära? 


#### **checkpoint 10/04/24**

TESTA De senaste endpointsen.

Gör scriptet redo för deplyment. anpassa det för dockers.. .? 




### **DO DOOS 16/04/24** 

- Se till att queries funkar igen.... börjar med 30 minuter. 
        - Det jag har upptäckt är att CREATION_DATE är i UTC vilket gör att det ser konstigt ut i loggen. 
        - Kanske får databasen att spara allt i UTC.. det ser ju ut som ett jätteglapp i datan när man jämför insertion_date och creation_date. 

- Fixa så att om requesten går över max limit pausa i 1h. 30?? 

- Fixa endpoints so de går att söka baserat på en tidsperiod också. 


right now 10:40 17/6/24. KÖRS **fetch_insert_loop copy** för att se om fail safe functinen funkar... 
    _ we are running it with a added function to the fetch_satellite_data()
    and the try_reconnet() function.

    2024-04-17 21:33:31,665 - INFO - Skipped 0 duplicate data points.
2024-04-17 21:42:59,633 - INFO - Logged in successfully to space-track.org.
2024-04-17 21:42:59,851 - ERROR - Failed to fetch satellite data: 502 Server Error: Bad Gateway for url: https://www.space-track.org/basicspacedata/query/class/gp/creation_date/%3Enow-0.0034722/orderby/norad_cat_id/format/json
2024-04-17 21:42:59,861 - ERROR - An error occurred: 502 Server Error: Bad Gateway for url: https://www.space-track.org/basicspacedata/query/class/gp/creation_date/%3Enow-0.0034722/orderby/norad_cat_id/format/json
2024-04-17 21:47:00,645 - INFO - Logged in successfully to space-track.org.
2024-04-17 21:47:00,929 - INFO - Satellite data fetched successfully.
2024-04-17 21:47:00,992 - INFO - Inserted NORAD_CAT_ID: 59289 with SpaceTrack creation date in UTC: 2024-04-17T19:42:57
2024-04-17 21:47:00,992 - INFO - Inserted NORAD_CAT_ID: 59290 with Sp

vad kan bad gateway betyda?? 


## **18/05/24** 

jag jobbar på: 

endpoints: 
    - alerting system. 
        det är byggt på creation date, och att den ska kunna identifiera om en NORAD har ett för gammalt creation date. detta var ju såklart dumt eftersom alla har super gamla creation dates.. .bättre är nog att ta vår creation date..
            - eller snarare att den ska ge en alert på om den senaste creation date är äldre än 4 dagar... och räkna dessa, skicka en lista. 
        DETTA ÄR NYTT: skapa ett skrip som håller koll på hur amla datan är och som du kan kalla på via endpoint eller få tag på via endpoint. 


queryn... 
    - jag får inte ihoop det med queryn.. testar i tst_query så långt ner det går, vi vet att 10 minuter funkar utmärkt men detta kan vara en limit... 


    There is also a lot of test function in fecth loop copy. 
        - try the sound fetching. 


        